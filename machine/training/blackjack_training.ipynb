{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68.2 ms, sys: 8.45 ms, total: 76.6 ms\n",
      "Wall time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import pickle, gzip, urllib.request, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "import io\n",
    "import struct\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket='sagemaker-blackjack' # Replace with your s3 bucket name\n",
    "prefix = 'sagemaker/blackjack' # Used as part of the path in the bucket where you store data\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket) # The URL to access the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-blackjack/sagemaker/blackjack/train\n",
      "s3://sagemaker-blackjack/sagemaker/blackjack/xgboost_model_sdk\n"
     ]
    }
   ],
   "source": [
    "train_data = 's3://{}/{}/{}'.format(bucket, prefix, 'train')\n",
    "print(train_data)\n",
    "\n",
    "s3_output_location = 's3://{}/{}/{}'.format(bucket, prefix, 'xgboost_model_sdk')\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, # number of ML compute instances to use for training\n",
    "                                         train_instance_type='ml.m4.xlarge', # type of ML computer instance for training\n",
    "                                         train_volume_size = 1, # size of storage to attach to training instance\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sagemaker.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html\n",
    "xgb_model.set_hyperparameters(max_depth = 5, # (Default: 6) Maximum depth of a tree. Increasing this value makes the model more complex and likely to be overfit. 0 indicates no limit.\n",
    "                              eta = .2, # (Default: 0.3) Step size shrinkage used in updates to prevent overfitting. After each boosting step, you can directly get the weights of new features. The eta parameter actually shrinks the feature weights to make the boosting process more conservative.\n",
    "                              gamma = 4, # (Default: 0) Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm is.\n",
    "                              min_child_weight = 6, # (Default: 1) Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning. In linear regression models, this simply corresponds to a minimum number of instances needed in each node. The larger the algorithm, the more conservative it is.\n",
    "                              silent = 0, # (Default: 0) 0 means print running messages, 1 means silent mode.\n",
    "                              objective = \"multi:softmax\", # (Default: reg:squarederror) Specifies the learning task and the corresponding learning objective. Examples: reg:logistic, multi:softmax, reg:squarederror. For a full list of valid inputs, refer to XGBoost Parameters (https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst).\n",
    "                              num_class = 12, # The number of classes. Required if objective is set to multi:softmax or multi:softprob.\n",
    "                              num_round = 10) # The number of rounds to run the training. Required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.session.s3_input(train_data, content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel} # can also add validation channel here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-31 20:14:14 Starting - Starting the training job...\n",
      "2020-03-31 20:14:15 Starting - Launching requested ML instances......\n",
      "2020-03-31 20:15:15 Starting - Preparing the instances for training...\n",
      "2020-03-31 20:16:07 Downloading - Downloading input data...\n",
      "2020-03-31 20:16:25 Training - Downloading the training image...\n",
      "2020-03-31 20:16:55 Training - Training image download completed. Training in progress.\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value multi:softmax to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[20:16:57] 2501256x2 matrix with 5002512 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 2501256 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-merror:0.60371\u001b[0m\n",
      "\u001b[34m[1]#011train-merror:0.596739\u001b[0m\n",
      "\u001b[34m[2]#011train-merror:0.590699\u001b[0m\n",
      "\u001b[34m[3]#011train-merror:0.585564\u001b[0m\n",
      "\u001b[34m[4]#011train-merror:0.58367\u001b[0m\n",
      "\u001b[34m[5]#011train-merror:0.581655\u001b[0m\n",
      "\u001b[34m[6]#011train-merror:0.579526\u001b[0m\n",
      "\u001b[34m[7]#011train-merror:0.579152\u001b[0m\n",
      "\n",
      "2020-03-31 20:18:12 Uploading - Uploading generated training model\u001b[34m[8]#011train-merror:0.577729\u001b[0m\n",
      "\u001b[34m[9]#011train-merror:0.574415\u001b[0m\n",
      "\n",
      "2020-03-31 20:18:19 Completed - Training job completed\n",
      "Training seconds: 132\n",
      "Billable seconds: 132\n"
     ]
    }
   ],
   "source": [
    "# Start model training\n",
    "xgb_model.fit(inputs=data_channels,  logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model for testing\n",
    "xgb_predictor = xgb_model.deploy(initial_instance_count=1,\n",
    "                                content_type='text/csv',\n",
    "                                instance_type='ml.t2.medium'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'6.0'\n",
      "b'11.0'\n",
      "b'1.0'\n"
     ]
    }
   ],
   "source": [
    "# Test the deployed model\n",
    "result = xgb_predictor.predict('803,1108')\n",
    "print(result)\n",
    "\n",
    "# this data point accurately predicts a split!\n",
    "result = xgb_predictor.predict('803,1111')\n",
    "print(result)\n",
    "\n",
    "result = xgb_predictor.predict('203,1007')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
